[
  {
    "objectID": "proposal.html",
    "href": "proposal.html",
    "title": "Proposal",
    "section": "",
    "text": "Not writing formally yet."
  },
  {
    "objectID": "proposal.html#introduction",
    "href": "proposal.html#introduction",
    "title": "Proposal",
    "section": "Introduction",
    "text": "Introduction\nIn this project, we want to try to identify empirical diagnostic criteria for poorly identified models and demonstrate this with a couple examples in different frameworks. I will be working on this goal in the context of multilevel models.\nMultilevel models can have issues with identification just like any other. These often show up as model convergence issues when trying to fit a model, especially those with many varying effects. Researchers then might cite the lack of model convergence as their reason for not proceeding with that model, but the underlying issue may really be1 one of model identifiability. Model convergence alone is not the greatest reason to put aside a model2, but being able to point to a lack of identifiability may be more constructive—at least acknowledging the underlying reason why the model did not converge."
  },
  {
    "objectID": "proposal.html#methods",
    "href": "proposal.html#methods",
    "title": "Proposal",
    "section": "Methods",
    "text": "Methods\nOur high-level strategy here is to examine the properties of well- and poorly-identified models to identify differences that are therefore indicative of poor identification. Using a large dataset, we will first fit a model of interest, assuming that the size of the dataset will mean the model is identified. For the most relevance, the model of interest might ideally be one known to be prone to identification issues. Then, we will induce a lack of identifiability by subsetting the dataset into smaller versions and fitting the model of interest with the subsets. We will compare the identified and unidentified models by examining bootstrapped sampling distributions of their estimates and convergence warnings.\n\nLarge Dataset\nWe will use publicly available data from the Trends in International Mathematics and Science Study (TIMSS) in this example. This large dataset contains lots of information on student, teacher, and school variables. We will use the 2023 data for 8th graders in the United States. Broadest population within a single data file. Could go larger (including other countries or 6th grade as well). But using this as a starting point\n\n\nCode\nload(here::here(\"..\", \"Data\", \"TIMSS\", \"bsgusam8.rdata\")) # student context data\nload(here::here(\"..\", \"Data\", \"TIMSS\", \"bcgusam8.rdata\")) # school context data\n\ndat &lt;- BSGUSAM8 |&gt; select(school_id = IDSCHOOL, class_id = IDCLASS, student_id = IDSTUD,\n            starts_with(\"BSMMAT0\"), sex = ITSEX, student_likes_math = BSDGSLM) |&gt;\n  rename_with(~ paste0(\"math_pv_\", parse_number(.x)), starts_with(\"BSMMAT0\")) |&gt;\n  left_join(BCGUSAM8 |&gt;\n              select(school_id = IDSCHOOL, school_socioeconomic = BCDGSBC),\n            by = \"school_id\") |&gt;\n  mutate(across(everything(), ~ case_when(.x == attr(.x, \"na_values\") ~ NA, \n                                          .default = .x)),\n         across(c(student_likes_math, school_socioeconomic), ~ factor(.x, ordered = F)),\n         male = factor(sex, levels = 1:2, labels = c(\"female\", \"male\"))) |&gt;\n  zap_labels() |&gt;\n  select(-sex) |&gt;\n  drop_na()\n\nn_schools &lt;- dat |&gt; pull(school_id) |&gt; unique() |&gt; length()\nn_students &lt;- dat |&gt; pull(student_id) |&gt; unique() |&gt; length()\n\nn_info &lt;- dat |&gt;\n  summarize(.by = school_id,\n            n_students = n()) |&gt;\n  summarize(mean_n_students = mean(n_students),\n            sd_n_students = sd(n_students),\n            min_n_students = min(n_students),\n            max_n_students = max(n_students))\n\ndat |&gt;\n  select(-c(school_id, class_id, student_id)) |&gt;\n  mutate(school_socioeconomic = factor(school_socioeconomic,\n                                       labels = c(\"1\" = \"More Affluent\",\n                                                  \"2\" = \"Neither More Affluent nor More Disadvantaged\",\n                                                  \"3\" = \"More Disadvantaged\")),\n         student_likes_math = factor(student_likes_math,\n                                     labels = c(\"1\" = \"Very Much Like Learning Mathematics\",\n                                                \"2\" = \"Somewhat Like Learning Mathematics\",\n                                                \"3\" = \"Do Not Like Learning Mathematics\")),\n         male = factor(male,\n                       labels = c(\"male\" = \"Male\",\n                                  \"female\" = \"Female\"))) |&gt;\n  gtsummary::tbl_summary(label = list(student_likes_math = \"Student Likes Math\",\n                                      school_socioeconomic = \"School Socioeconomic Status\",\n                                      male = \"Gender\")) |&gt;\n  gtsummary::as_gt() |&gt;\n  gt::tab_footnote(\"The plausible values are five imputed values for each student's score. I think ideally these would be modeled in an imputation way, but for now I've just used the 1st PV as the outcome.\") |&gt;\n  gt::tab_options(table.width = pct(66))\n\n\n\n\n\n  \n    \n      Characteristic\n      N = 7,0271\n    \n  \n  \n    1ST PLAUSIBLE VALUE MATHEMATICS\n493 (429, 560)\n    2ND PLAUSIBLE VALUE MATHEMATICS\n493 (429, 561)\n    3RD PLAUSIBLE VALUE MATHEMATICS\n494 (428, 562)\n    4TH PLAUSIBLE VALUE MATHEMATICS\n492 (427, 561)\n    5TH PLAUSIBLE VALUE MATHEMATICS\n493 (428, 562)\n    Student Likes Math\n\n        Very Much Like Learning Mathematics\n1,027 (15%)\n        Somewhat Like Learning Mathematics\n2,365 (34%)\n        Do Not Like Learning Mathematics\n3,635 (52%)\n    School Socioeconomic Status\n\n        More Affluent\n840 (12%)\n        Neither More Affluent nor More Disadvantaged\n2,602 (37%)\n        More Disadvantaged\n3,585 (51%)\n    Gender\n\n        Male\n3,449 (49%)\n        Female\n3,578 (51%)\n  \n  \n    \n       The plausible values are five imputed values for each student's score. I think ideally these would be modeled in an imputation way, but for now I've just used the 1st PV as the outcome.\n    \n    \n      1 Median (Q1, Q3); n (%)\n    \n  \n\n\n\n\nThe full dataset contains complete data (on the variables in the model) for 189 schools, 402 classrooms, and 7027 students.\n\n\nCode\nn_info |&gt;\n  gt() |&gt;\n  cols_label(mean_n_students = \"Mean\",\n             sd_n_students = \"SD\",\n             min_n_students = \"Min\", \n             max_n_students = \"Max\") |&gt;\n  tab_header(\"Students Per School\") |&gt;\n  fmt_number() |&gt;\n  tab_style(style = cell_text(weight = \"bold\"),\n            cells_column_labels())\n\n\n\n\n\n\n  \n    \n      Students Per School\n    \n    \n    \n      Mean\n      SD\n      Min\n      Max\n    \n  \n  \n    37.18\n11.55\n4.00\n66.00\n  \n  \n\n\n\nOn average, each school has 37.18 students. Three schools have less than 10 students (4, 5, and 6 students).\n\n\nModel of Interest\nAs an example model of interest, we can examine the population effects of student gender, how much students report liking math, and school socioeconomic status on student math achievement scores with a two-level model of students nested within schools. We can also examine whether the student-level effects of gender and reporting liking math effects vary among schools, for two varying slope terms.\nWe can represent the unconditional form of this model as\n\n\\[\\begin{gathered}\n  \\begin{aligned}\n    \\text{Level 1}: & \\ \\text{MATH SCORE}_{ij} = \\beta_{0j} + r_i \\\\[.8em]\n    \\text{Level 2}: & \\ \\beta_{0j} = \\ \\gamma_{00} + u_{0j}\n  \\end{aligned}\n\n  \\\\[1.6em]\n\n  \\text{MATH SCORE}_{ij} = \\gamma_{00} + u_{0j} + r_i\n\\end{gathered}\\]\n\nand the conditional form of this model as\n\n\\[\\begin{gathered}\n  \\begin{aligned}\n    \\text{Level 1}: & \\ \\text{MATH SCORE}_{ij} = \\beta_{0j} + \\beta_{1j} \\text{GENDER}_i + \\beta_{2j} \\text{LIKES MATH}_i + r_i \\\\[.8em]\n    \\text{Level 2}: &\n      \\begin{aligned}\n        \\beta_{0j} =& \\ \\gamma_{00} + \\gamma_{01} \\text{SOCIOECONOMIC}_j + u_{0j} \\\\\n        \\beta_{1j} =& \\ \\gamma_{10} + u_{1j} \\\\\n        \\beta_{2j} =& \\ \\gamma_{20} + u_{2j}\n      \\end{aligned}\n  \\end{aligned}\n\n  \\\\[1.6em]\n\n  \\text{MATH SCORE}_{ij} = \\gamma_{00} + \\gamma_{01} \\text{SOCIOECONOMIC}_j + u_{0j} + (\\gamma_{10} + u_{1j}) \\text{GENDER}_i + (\\gamma_{20} + u_{2j}) \\text{LIKES MATH}_i + r_i\n\\end{gathered}\\]\n\n.\n\n\n\n\n\n\nDistributional Notation\n\n\n\n\n\nIf you’ll indulge my trying to practice translating these things into a distributional form.\n\n\\[\\begin{aligned}\n  \\text{MATH SCORE}_{i} &= \\alpha_{school[i]} + {\\beta_{female}}_{school[i]} \\text{female}_i + {\\beta_{likesMath}}_{school[i]} \\text{likesMath}_i\\\\\n  \\alpha_{school[i]} &\\sim \\mathcal{N}(\\bar{\\alpha}_j, \\sigma_{school[i]}) \\\\\n  {\\beta_{female}}_{school[i]}  &\\sim \\mathcal{N}({\\bar{\\beta}_{female}}_{school[i]}, {\\sigma_{female}}_{school[i]}) \\\\\n  {\\beta_{likesMath}}_{school[i]}  &\\sim \\mathcal{N}({\\bar{\\beta}_{likesMath}}_{school[i]}, {\\sigma_{likesMath}}_{school[i]}) \\\\\n\n\\begin{bmatrix}\n\\alpha_j & {\\beta_j}_{female} & {\\beta_j}_{likesMath}\n\\end{bmatrix}\n\n&\\sim\n\n\\mathcal{MVN}(\\begin{bmatrix}\n\\bar{\\alpha} & \\bar{\\beta}_{female} & \\bar{\\beta}_{likesMath}\n\\end{bmatrix},\nR,\n\\begin{bmatrix}\n{\\tau}_{\\alpha} & {\\tau_{\\beta}}_{female} & {\\tau_{\\beta}}_{likesMath}\n\\end{bmatrix}) \\\\\n\n  \\bar{\\alpha}, \\bar{\\beta}_{female}, \\bar{\\beta}_{likesMath} &\\sim \\mathcal{N}(0, 1) \\\\\n  {\\tau}_{\\alpha}, {\\tau_{\\beta}}_{female}, {\\tau_{\\beta}}_{likesMath} &\\sim \\mathcal{Uniform}(0, 400)\\\\\n  R &\\sim \\mathcal{LKJCorr}(4)\n\\end{aligned}\\]\n\nOr, centered3 parameterization for the covariance matrix.\n\\[\n\\begin{bmatrix}\n  \\alpha_j \\\\\n  {\\beta_j}_{female}\\\\\n  {\\beta_j}_{likesMath}\n\\end{bmatrix} \\sim \\mathcal{MVN}(\n  \\begin{bmatrix}\n    \\bar{\\alpha} \\\\\n    \\bar{\\beta}_{female} \\\\\n    \\bar{\\beta}_{likesMath}\n  \\end{bmatrix}, \\Sigma=\n  \\begin{bmatrix}\n    {\\tau}_{\\alpha}^2 & \\tau_{\\alpha \\beta_{female}} & \\tau_{\\alpha \\beta_{likesMath}} \\\\\n    \\tau_{\\beta_{female} \\alpha} & {\\tau}_{\\beta_{female}}^2 & \\tau_{\\beta_{female}\\beta_{likesMath}} \\\\\n    \\tau_{\\beta_{likesMath} \\alpha} & \\tau_{\\beta_{likesMath} \\beta_{female}} & {\\tau}_{\\beta_{likesMath}}^2\n\\end{bmatrix})\n\\]\n\n\n\n\n\nManipulating Identifiability\nBecause this dataset is so large, we can assume that the model fit on the full dataset is well-identified. We can then manipulate identifiability by randomly subsetting the data into smaller datasets. This represents what smaller research teams might be more likely to encounter in their work. We could also subset the data by narrowing the population, e.g., 8th graders in the Midwest.\nWe still need to decide what the sample sizes of the smaller datasets should be, considering both number of schools and number of students within schools.\n\n\nEmpirical Outcomes\nAfter manipulating identifiability, we will examine how the model reacts to having fewer observations with which to estimate its parameters. As one approach to empirically diagnosing poor identifiability, we will examine bootstrapped sampling distributions for parameters to examine them for issues. We will also track warnings that the model did not converge. We want to do at least part of this with MCMC, right? So we can also look at the posterior distributions in that way?\nWe migh also be able to show that by moving to a Bayesian framework with even gentle priors, identifiability issues are reduced. So like {lme4} → {brms} with flat priors → {brms} with weakly informative priors? Or {lme4} → {brms} with weakly informative priors."
  },
  {
    "objectID": "proposal.html#results",
    "href": "proposal.html#results",
    "title": "Proposal",
    "section": "Results",
    "text": "Results\n\n\nCode\nn_bootstraps &lt;- 100\n\n\nJust starting to build up the code for this.\n\nComplete Data\nBelow are the distributions and correlations for the bootstrapped parameter estimates (100 bootstraps) for the model fit on the full data. Just displaying fixed effects right now to get started (so not the more interesting pieces yet I guess).\n\n\nCode\nbootstr &lt;- dat |&gt;\n  # filter(school_id %in% sample(school_id, 4)) |&gt;\n  # slice_sample(by = school_id, n = 8) |&gt;\n  modelr::bootstrap(n_bootstraps, id = \"id\")\n\nbootres &lt;- bootstr |&gt;\n  mutate(lm = map(strap, ~ lme4::lmer(math_pv_1 ~ male + student_likes_math + school_socioeconomic + \n                                        (male + student_likes_math | school_id),\n                                      data = .x)),\n         tidy = map(lm, broom.mixed::tidy),\n         opt_warnings = map_chr(lm, \n                                ~ ifelse(.x |&gt; pluck(\"optinfo\") |&gt; pluck(\"warnings\") |&gt; length() == 0, NA,\n                                .x |&gt; pluck(\"optinfo\") |&gt; pluck(\"warnings\"))),\n         lme4_warnings = map_chr(lm, \n                                 ~ ifelse(.x |&gt; pluck(\"optinfo\") |&gt; pluck(\"conv\") |&gt; pluck(\"lme4\") |&gt;\n                                            pluck(\"messages\") |&gt; pluck(1) |&gt; is_null(), NA,\n                                          .x |&gt; pluck(\"optinfo\") |&gt; pluck(\"conv\") |&gt; pluck(\"lme4\") |&gt;\n                                            pluck(\"messages\") |&gt; pluck(1))))\n\nbootest &lt;- bootres |&gt;\n  pull(tidy) |&gt;\n  map2(seq(1, bootres |&gt; pull(id) |&gt; parse_number() |&gt; max()), \n       ~ mutate(.x, iter = .y)) |&gt;\n  list_rbind() |&gt;\n  select(group, term, estimate, iter) \n\n\n\n\nCode\nterms &lt;- bootest |&gt; pull(term) |&gt; unique() |&gt; head(6)\n\nimap(terms, \\(yterm, yid_term) \n     imap(terms |&gt; head(yid_term),\n          \\(xterm, xid_term) if (xterm != yterm){\n            bootest |&gt;\n              filter(term == xterm | term == yterm) |&gt;\n              pivot_wider(names_from = term, values_from = estimate) |&gt;\n              ggplot(aes(x = !!sym(xterm), y = !!sym(yterm))) +\n              geom_point() +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = NULL) +\n              labs(x = if (yid_term == length(terms)) xterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10) else NULL,\n                   y = yterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10))}\n          else {bootest |&gt;\n              filter(term == yterm) |&gt;\n              ggplot(aes(x = estimate)) +\n              geom_density() +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = NULL) +\n              labs(x = if (yid_term == length(terms)) xterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10) else NULL,\n                   y = yterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10))}) |&gt;\n       patchwork::wrap_plots(axis_titles = \"collect\", nrow = 1, widths = rep(1, length(terms))) &\n       theme_classic()\n) |&gt;\n  patchwork::wrap_plots(axis_titles = \"collect\", ncol = 1, heights = rep(1, length(terms))) \n\n\n\n\n\n\n\n\n\n\n\nSubsetted Data\nAnd here’s the same thing with a subset of 5 schools and 10 students within each school.\n\n\nCode\nbootstr &lt;- dat |&gt;\n  filter(school_id %in% sample(school_id, 5)) |&gt;\n  slice_sample(by = school_id, n = 10) |&gt;\n  modelr::bootstrap(n_bootstraps, id = \"id\")\n\nbootres &lt;- bootstr |&gt;\n  mutate(lm = map(strap, ~ lme4::lmer(math_pv_1 ~ male + student_likes_math + school_socioeconomic + \n                                        (male + student_likes_math | school_id),\n                                      data = .x)),\n         tidy = map(lm, broom.mixed::tidy),\n         opt_warnings = map_chr(lm, \n                                ~ ifelse(.x |&gt; pluck(\"optinfo\") |&gt; pluck(\"warnings\") |&gt; length() == 0, NA,\n                                .x |&gt; pluck(\"optinfo\") |&gt; pluck(\"warnings\"))),\n         lme4_warnings = map_chr(lm, \n                                 ~ ifelse(.x |&gt; pluck(\"optinfo\") |&gt; pluck(\"conv\") |&gt; pluck(\"lme4\") |&gt;\n                                            pluck(\"messages\") |&gt; pluck(1) |&gt; is_null(), NA,\n                                          .x |&gt; pluck(\"optinfo\") |&gt; pluck(\"conv\") |&gt; pluck(\"lme4\") |&gt;\n                                            pluck(\"messages\") |&gt; pluck(1))))\n\nbootest &lt;- bootres |&gt;\n  pull(tidy) |&gt;\n  map2(seq(1, bootres |&gt; pull(id) |&gt; parse_number() |&gt; max()), \n       ~ mutate(.x, iter = .y)) |&gt;\n  list_rbind() |&gt;\n  select(group, term, estimate, iter) \n\n\n\n\nCode\nterms &lt;- bootest |&gt; filter(str_detect(term, dat |&gt; names() |&gt; paste(collapse = \"|\")) | \n                    str_detect(term, \"Intercept\")) |&gt;\n  filter(!str_detect(term, \"sd|cor\")) |&gt;\n  pull(term) |&gt; unique()\n\nimap(terms, \\(yterm, yid_term) \n     imap(terms |&gt; head(yid_term),\n          \\(xterm, xid_term) if (xterm != yterm){\n            bootest |&gt;\n              filter(term == xterm | term == yterm) |&gt;\n              pivot_wider(names_from = term, values_from = estimate) |&gt;\n              ggplot(aes(x = !!sym(xterm), y = !!sym(yterm))) +\n              geom_point() +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = NULL) +\n              labs(x = if (yid_term == length(terms)) xterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10) else NULL,\n                   y = yterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10))}\n          else {bootest |&gt;\n              filter(term == yterm) |&gt;\n              ggplot(aes(x = estimate)) +\n              geom_density() +\n              scale_y_continuous(breaks = NULL) +\n              scale_x_continuous(breaks = NULL) +\n              labs(x = if (yid_term == length(terms)) xterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10) else NULL,\n                   y = yterm |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(10))}) |&gt;\n       patchwork::wrap_plots(axis_titles = \"collect\", nrow = 1, widths = rep(1, length(terms))) &\n       theme_classic()\n) |&gt;\n  patchwork::wrap_plots(axis_titles = \"collect\", ncol = 1, heights = rep(1, length(terms)))"
  },
  {
    "objectID": "proposal.html#archive",
    "href": "proposal.html#archive",
    "title": "Proposal",
    "section": "Archive",
    "text": "Archive\n\n\nCode\nterms &lt;- bootest |&gt; pull(term) |&gt; unique() |&gt; head(3)\n\nimap(terms, \\(yterm, yid_term) \n     imap(terms |&gt; head(yid_term),\n          \\(xterm, xid_term) if (xterm == yterm) {\n            bootest |&gt;\n              filter(term == xterm) |&gt;\n              ggplot(aes(x = estimate)) +\n              geom_density() +\n              labs(x = xterm, y = xterm) +\n              theme(aspect.ratio = 1)\n          } else {\n            bootest |&gt;\n              filter(term == xterm | term == yterm) |&gt;\n              pivot_wider(names_from = term, values_from = estimate) |&gt;\n              ggplot(aes(x = !!sym(xterm), y = !!sym(yterm))) +\n              geom_point() +\n              labs(x = xterm, y = yterm) +\n              theme(aspect.ratio = 1)\n          }) |&gt;\n       patchwork::wrap_plots(ncol = length(terms), byrow = T, axis_titles = \"collect\")\n)\n\n\nyid_term &lt;- 3\nyterm &lt;- terms[yid_term]\n\nmap(terms |&gt; head(yid_term),\n    \\(xterm) if (xterm != yterm){\n      bootest |&gt;\n        filter(term == xterm | term == yterm) |&gt;\n        pivot_wider(names_from = term, values_from = estimate) |&gt;\n        ggplot(aes(x = !!sym(xterm), y = !!sym(yterm))) +\n        geom_point() +\n        labs(x = xterm, y = yterm)}\n    else {bootest |&gt;\n        filter(term == yterm) |&gt;\n        ggplot(aes(x = estimate)) +\n        geom_density() +\n        labs(x = yterm, y = yterm)}) |&gt;\n  patchwork::wrap_plots(axis_titles = \"collect\")\n\n    map(iris$Species |&gt; unique(),\n    \\(sp) if (sp != \"setosa\") \n    {iris |&gt;\n        filter(Species == \"setosa\" | Species == sp) |&gt;\n        ggplot(aes(x = Petal.Length, y = Sepal.Length)) +\n        geom_point()\n    } else \n    {iris |&gt;\n        filter(Species == \"setosa\") |&gt;\n        ggplot(aes(x = Petal.Length, y = Sepal.Length)) +\n        geom_point()}) |&gt;\n  patchwork::wrap_plots(axis_titles = \"collect\")\n\nmap(iris$Species |&gt; unique(),\n    \\(sp) iris |&gt;\n        filter(Species == \"setosa\" | Species == sp) |&gt;\n        ggplot(aes(x = Petal.Length, y = Sepal.Length)) +\n        geom_point()) |&gt;\n  patchwork::wrap_plots(axis_titles = \"collect\") &\n  theme_classic()\n# expand_grid(x = terms, y = terms) |&gt;\n#   pmap(\\(x, y) if (x == y) {\n#     bootest |&gt;\n#       filter(term == x) |&gt;\n#       ggplot(aes(x = estimate)) +\n#       geom_density() +\n#       labs(x = x, y = x)\n#   } else {\n#     bootest |&gt;\n#       filter(term == x | term == y) |&gt;\n#       pivot_wider(names_from = term, values_from = estimate) |&gt;\n#       ggplot(aes(x = !!sym(x), y = !!sym(y))) +\n#       geom_point()+\n#       labs(x = x, y = y)\n#   }) |&gt;\n#   patchwork::wrap_plots(axis_titles = \"collect\", guides = \"collect\", byrow = F) &\n#   theme(aspect.ratio = 1)\n# \n# bootest |&gt;\n#       filter(term == \"(Intercept)\" | term == y) |&gt;\n#       pivot_wider(names_from = term, values_from = estimate) |&gt;\n#       ggplot(aes(x = \"(Intercept)\", y = y)) +\n#       geom_point()\n# \n# \n# map(bootest |&gt; pull(term) |&gt; unique(), \n#     ~ bootest |&gt; \n#       filter(term == .x) |&gt;\n#       ggplot(aes(x = estimate)) +\n#       geom_density() +\n#       labs(title = .x |&gt; str_replace_all(\"_\", \" \") |&gt; str_wrap(width = 15)) +\n#       theme_classic() +\n#       theme(axis.text = element_blank(),\n#             plot.title = element_text(hjust = 0.5, size = 8))) |&gt;\n#   patchwork::wrap_plots(axes = \"collect\") +\n#   patchwork::plot_annotation()"
  },
  {
    "objectID": "proposal.html#footnotes",
    "href": "proposal.html#footnotes",
    "title": "Proposal",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nis?↩︎\nwhy?↩︎\nnon-centered?↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Empirical Identifiability",
    "section": "",
    "text": "Page for empirical identifiability project."
  }
]